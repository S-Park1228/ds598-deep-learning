%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{threeparttable}
\usepackage{lipsum}
\usepackage{multirow}
\newcommand\tab[1][1cm]{\hspace*{#1}}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------
\title{DS 598 DEEP LEARNING}
\author{Sungjoon Park (BUID: U38522578)}
\date{Feb. 8th, 2024}

\begin{document}
\maketitle

\setlength{\parindent}{0cm}
\setlength{\parskip}{2mm}

%----------------------------------------------------------------------------------------
%	BODY
%----------------------------------------------------------------------------------------
\textbf{Knowledge Check 5}

\begin{enumerate}

% Problem 1
\item (Problem 5.2) \\
	$<$ (i) when the training label $y=0$ $>$
	\begin{align*}
	L &= -\log[1-sig[f[x, \phi]]] \\
	\end{align*}
	
	\begin{center}
		\begin{figure}[h]	
		\includegraphics[scale=0.6]{ps5-1-1.png}
		\vspace*{-2mm}
		\caption{Loss as a function of the transformed network output when $y=0$}
		\end{figure}
	\end{center}
	
	$<$ (ii) when the training label $y=1$ $>$ \\
	\begin{align*}
	L &= -\log[sig[f[x, \phi]]] \\
	\end{align*}
	
	\begin{center}
		\begin{figure}[h]	
		\includegraphics[scale=0.6]{ps5-1-2.png}
		\vspace*{-2mm}
		\caption{Loss as a function of the transformed network output when $y=1$}
		\end{figure}
	\end{center}

\

% Problem 2	
\item (Problem 5.3) \\
	Probability Distribution Function for the target $y$: $Pr(y|\mu, \kappa)=\frac{\exp[\kappa\cos[y-\mu]]}{2\pi\cdot \text{Bessel}_0[\kappa]}$ \\
	Distribution Parameter: $\mu$ \\
	Constant (Exogenous variable): $\kappa$ \\
	Machine learning model: $\mu=f[\textbf{x}, \phi]$ $\rightarrow$ $Pr(y|\mu, \kappa)=Pr(y|f[\textbf{x}, \phi], \kappa)$ \\
	By entering the probability distribution of the target $y$, we can construct a negative log-likelihood loss function like below over the training dataset pairs $\{x_i, y_i\}$. \\
	Loss function:
	\begin{align*}
	L(\phi) &= -\sum_{i=1}^I \log[Pr(y_i|f[\textbf{x}_i, \phi], \kappa)] \\
	&= -\sum_{i=1}^I \log[\frac{\exp[\kappa\cos[y_i-\mu_i]]}{2\pi\cdot \text{Bessel}_0[\kappa]}] \\
	&= -\sum_{i=1}^I [\kappa\cos[y_i-\mu_i]]+\sum_{i=1}^I \log[2\pi\cdot \text{Bessel}_0[\kappa]] \\
	\end{align*}
	Since the second term in the right-hand side is constant, the loss function can be simplified as follows.
	\begin{align*}
	L(\phi) &= -\sum_{i=1}^I [\kappa\cos[y_i-\mu_i]] \\
	&= -\sum_{i=1}^I [\kappa\cos[y_i-f[\textbf{x}_i, \phi]]]
	\end{align*}

\
	
% Problem 3
\item (Problem 5.6) \\
	Probability Distribution Function for the target $y$: $Pr(y=k|\lambda)=\frac{\lambda^k \exp(-\lambda)}{k!}$ \\
	Distribution Parameter: $\lambda$ \\
	Machine learning model: $\lambda=f[\textbf{x}, \phi]$ $\rightarrow$ $Pr(y|\lambda)=Pr(y|f[\textbf{x}, \phi])$ \\
	Since $\lambda$ must be non-negative, the output layer of the network $f$ should consider it. By entering the probability distribution of the target $y$, we can construct a negative log-likelihood loss function like below over the training dataset pairs $\{x_i, y_i\}$. \\
	Loss function:
	\begin{align*}
	L(\phi) &= -\sum_{i=1}^I \log[Pr(y_i|f[\textbf{x}_i, \phi])] \\
	&= -\sum_{i=1}^I \log[\frac{f[\textbf{x}_i, \phi]^{y_i} \exp(-f[\textbf{x}_i, \phi])}{y_i!}] \\
	&= -\sum_{i=1}^I y_i \log[f[\textbf{x}_i, \phi]]+\sum_{i=1}^I f[\textbf{x}_i, \phi]+\sum_{i=1}^I \log[y_i!]
	\end{align*}

\

% Problem 4
\item (Problem 5.7) \\
	Considering the independence of $\textbf{y}$, \\
	($i$ stands for the indices of observations and $j$ stands for indices of elements of $\textbf{y}_i$.) \\
	Likelihood: $\prod_{i=1}^N \prod_{j=1}^{10} \frac{1}{\sqrt{2\pi\sigma^2}}\exp[-\frac{(y_{ij}-f[\textbf{x}_{ij}, \phi])^2}{2\sigma^2}]$ \\
	Since constant terms, $\sum_{i=1}^N \sum_{j=1}^{10}\log[\frac{1}{\sqrt{2\pi\sigma^2}}]$ and $\sum_{i=1}^N \sum_{j=1}^{10}2\sigma^2$, can be ignored, \\
	Negative Log-likelihood: $-\sum_{i=1}^N \sum_{j=1}^{10} [-(y_{ij}-f[\textbf{x}_{ij}, \phi])^2]=\sum_{i=1}^N \sum_{j=1}^{10} (y_{ij}-f[\textbf{x}_{ij}, \phi])^2$ \\
	It is the same as the objective function of least squares, which is mean squared error (MSE) like below. \\
	MSE: $\sum_{i=1}^N \sum_{j=1}^{10} \epsilon_{ij}^2=\sum_{i=1}^N \sum_{j=1}^{10} (y_{ij}-f[\textbf{x}_{ij}, \phi])^2$, where $y_{ij}=f[\textbf{x}_{ij}, \phi]+\epsilon_{ij}$
	

\

\end{enumerate}

\end{document}